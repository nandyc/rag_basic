{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlWSDThKdsadSj/AcfSpKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandyc/rag_basic/blob/main/rag_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "f8hS9uTn3CWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# github_email = userdata.get('GITHUB_EMAIL')\n",
        "# github_username = userdata.get('GITHUB_USERNAME')"
      ],
      "metadata": {
        "id": "HAWhbKZY3crF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email {github_email}\n",
        "!git config --global user.name {github_username}"
      ],
      "metadata": {
        "id": "5LDj6mHrsUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_iux2Vu5P25",
        "outputId": "c04e3d7e-b8de-4913-8b88-12fb8ca0835f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rag_basic'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 27 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (27/27), 24.91 KiB | 8.30 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nandyc/rag_basic.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd rag_basic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoNlhRDm34lY",
        "outputId": "4fb0c97a-c0e7-49d4-e760-a7fd27aeb4c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rag_basic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git status\n",
        "# !echo \"# Some dummy text\" >> new.md\n",
        "# !git add .\n",
        "# !git commit -m \"test\"\n",
        "# !git push origin main\n"
      ],
      "metadata": {
        "id": "fjnTJTum4aAZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# !pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "986fDkGC-6zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import collections\n",
        "# from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions"
      ],
      "metadata": {
        "id": "oBMTO4mY5tAr",
        "outputId": "b655e90f-084f-42fe-cdf8-7599904331a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1e91372b55ff>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# !pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "zrDF9PbOlE95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_dotenv()\n",
        "# openai_key =os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRDvJ6D0kfYy",
        "outputId": "e15deff1-f28b-4ae2-b6a3-312b90a1d6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key =userdata.get('OPENAI_APIKEY')"
      ],
      "metadata": {
        "id": "gTHziD60mtom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=openai_key, model_name=\"text-embedding-3-small\"\n",
        ")"
      ],
      "metadata": {
        "id": "oY-3pgg3kjXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Chroma client with persistence\n",
        "chroma_client = chromadb.PersistentClient(path=\"chroma_persistent_storage\")\n",
        "collection_name = \"document_qa_collection\"\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=collection_name, embedding_function=openai_ef\n",
        ")"
      ],
      "metadata": {
        "id": "CgXByh8qklKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=openai_key)"
      ],
      "metadata": {
        "id": "4oL5uc4akmdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resp = client.chat.completions.create(\n",
        "#     model=\"gpt-3.5-turbo\",\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": \"What is human life expectancy in the United States?\",\n",
        "#         },\n",
        "#     ],\n",
        "# )\n",
        "# print(resp.choices[0].message.content)\n",
        "# print (resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBAbzmnUkny5",
        "outputId": "bea8b1bf-b364-48a6-e33c-68f85498e077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of 2021, the average life expectancy in the United States is around 78.9 years. However, this can vary based on factors such as gender, race, and socio-economic status. It is important to note that life expectancy is an average and individual outcomes can vary.\n",
            "ChatCompletion(id='chatcmpl-A0Ob84paAz1J8OtPVTTPcv1gj7EVi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As of 2021, the average life expectancy in the United States is around 78.9 years. However, this can vary based on factors such as gender, race, and socio-economic status. It is important to note that life expectancy is an average and individual outcomes can vary.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724657918, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=58, prompt_tokens=27, total_tokens=85))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from a directory\n",
        "def load_documents_from_directory(directory_path):\n",
        "    print(\"==== Loading documents from directory ====\")\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(\n",
        "                os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\"\n",
        "            ) as file:\n",
        "                documents.append({\"id\": filename, \"text\": file.read()})\n",
        "    print(type(documents[0]))\n",
        "    # list_of_filenames = collections.Counter([i['id'] for i in documents]).keys()\n",
        "    list_of_filenames = [item['id'] for item in documents]\n",
        "    print(list_of_filenames)\n",
        "    return documents"
      ],
      "metadata": {
        "id": "MqWrHVHwkq1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Function to split text into chunks\n",
        "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - chunk_overlap\n",
        "    print(chunks)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "8hmwCV9sksVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Load documents from the directory\n",
        "directory_path = \"./news_articles\"\n",
        "documents = load_documents_from_directory(directory_path)"
      ],
      "metadata": {
        "id": "lK9OA3q_kt2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loaded {len(documents)} documents\")"
      ],
      "metadata": {
        "id": "zruASE-rku-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents into chunks\n",
        "chunked_documents = []\n",
        "for doc in documents:\n",
        "    chunks = split_text(doc[\"text\"])\n",
        "    print(\"==== Splitting docs into chunks ====\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunked_documents.append({\"id\": f\"{doc['id']}_chunk{i+1}\", \"text\": chunk})\n",
        "\n",
        "print(f\"Split documents into {len(chunked_documents)} chunks\")\n",
        "# print(chunked_documents)"
      ],
      "metadata": {
        "id": "-bN6DHEEkwcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate embeddings using OpenAI API\n",
        "def get_openai_embedding(text):\n",
        "    response = client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
        "    embedding = response.data[0].embedding\n",
        "    print(\"==== Generating embeddings... ====\")\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "o5EcpKk_kxuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for the document chunks\n",
        "for doc in chunked_documents:\n",
        "    print(\"==== Generating embeddings... ====\")\n",
        "    doc[\"embedding\"] = get_openai_embedding(doc[\"text\"])\n",
        "\n",
        "# print(doc[\"embedding\"])\n",
        "print(chunked_documents)"
      ],
      "metadata": {
        "id": "ddy4MghxkzC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert documents with embeddings into Chroma\n",
        "# for doc in chunked_documents:\n",
        "#     print(\"==== Inserting chunks into db;;; ====\")\n",
        "#     collection.upsert(\n",
        "#         ids=[doc[\"id\"]], documents=[doc[\"text\"]], embeddings=[doc[\"embedding\"]]\n",
        "#     )"
      ],
      "metadata": {
        "id": "c60wHyWZk0oQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}